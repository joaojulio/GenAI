{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_splitter\u001b[39;00m \u001b[39mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentence_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformerEmbeddings\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib\u001b[39;00m \u001b[39mimport\u001b[39;00m metadata\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m MRKLChain, ReActChain, SelfAskWithSearchChain\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCache\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     ConversationChain,\n\u001b[0;32m     10\u001b[0m     LLMBashChain,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     VectorDBQAWithSourcesChain,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\agents\\__init__.py:31\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m**Agent** is a class that uses an LLM to choose a sequence of actions to take.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     Agent,\n\u001b[0;32m     33\u001b[0m     AgentExecutor,\n\u001b[0;32m     34\u001b[0m     AgentOutputParser,\n\u001b[0;32m     35\u001b[0m     BaseMultiActionAgent,\n\u001b[0;32m     36\u001b[0m     BaseSingleActionAgent,\n\u001b[0;32m     37\u001b[0m     LLMSingleActionAgent,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_iterator\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutorIterator\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_toolkits\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     create_csv_agent,\n\u001b[0;32m     42\u001b[0m     create_json_agent,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     create_xorbits_agent,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\agents\\agent.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\u001b[1;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39myaml\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_iterator\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentExecutorIterator\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magent_types\u001b[39;00m \u001b[39mimport\u001b[39;00m AgentType\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatPDF:\n",
    "\n",
    "    def __init__(self, files: list, vectordb_path: str):\n",
    "        self.files = files\n",
    "        self.pages = []\n",
    "        self.documents = []\n",
    "        self.vectordb_path = vectordb_path\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        pages = []\n",
    "\n",
    "        for file in self.files:\n",
    "            loader = PyPDFLoader(file)\n",
    "            pages = loader.load()\n",
    "            self.pages.extend(pages)\n",
    "            print(f\"Loading file {file}\")\n",
    "\n",
    "        return len(self.files), len(self.pages)\n",
    "    \n",
    "    def split(self, chunk_size: int = 1500, chunk_overlap: int = 150):\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "        self.documents = text_splitter.split_documents(self.pages)\n",
    "\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def store(self):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.vectordb_path\n",
    "        )\n",
    "\n",
    "        vectordb.persist()\n",
    "\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def create_llm(self, temperature: float = 0.4):\n",
    "        \n",
    "            self.llm = LlamaCpp(model_path=\"../models/llama-2-7b-chat.ggmlv3.q4_0.bin\", verbose=True, n_ctx=2048, temperature=temperature)\n",
    "\n",
    "    def create_memory(self):\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "    def create_retriever(self):\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def create_chat_session(self):\n",
    "\n",
    "        PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to male up an answer. \n",
    "        Use three sentences maximum. Keep answer as concise as possible. \n",
    "        Always say \"thanks for asking! at the end of the answer.\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Helpful Answer:\"\"\"\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "        self.qa = ConversationalRetrievalChain.from_llm(\n",
    "            self.llm,\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory,\n",
    "            combine_docs_chain_kwargs={'prompt': QA_CHAIN_PROMPT}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../docs/Anexo_2_Descricao_Desafios_caracteristicas_e_especificidades_dos_desafios']\n",
    "vectordb_path = \"../docs/chroma\"\n",
    "\n",
    "chat = ChatPDF(files, vectordb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.load()\n",
    "chat.split()\n",
    "chat.get_embeddings()\n",
    "chat.store()\n",
    "chat.create_llm()\n",
    "chat.create_memory()\n",
    "chat.create_retriever()\n",
    "chat.create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end web app\n",
    "chat_history = []\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    def user(user_message, chat_history):\n",
    "        \n",
    "        # Get result from QA chain\n",
    "        result = chat.qa({\"question\": user_message, \"chat_history\": chat_history})\n",
    "        \n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append((user_message, result[\"answer\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
