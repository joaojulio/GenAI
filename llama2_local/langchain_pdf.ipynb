{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joao.falmeida\\.conda\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatPDF:\n",
    "\n",
    "    def __init__(self, files: list, vectordb_path: str):\n",
    "        self.files = files\n",
    "        self.pages = []\n",
    "        self.documents = []\n",
    "        self.vectordb_path = vectordb_path\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        pages = []\n",
    "\n",
    "        for file in self.files:\n",
    "            loader = PyPDFLoader(file)\n",
    "            pages = loader.load()\n",
    "            self.pages.extend(pages)\n",
    "            print(f\"Loading file {file}\")\n",
    "\n",
    "        return len(self.files), len(self.pages)\n",
    "    \n",
    "    def split(self, chunk_size: int = 1500, chunk_overlap: int = 150):\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "\n",
    "        self.documents = text_splitter.split_documents(self.pages)\n",
    "\n",
    "        return len(self.documents)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        self.embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def store(self):\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.vectordb_path\n",
    "        )\n",
    "\n",
    "        vectordb.persist()\n",
    "\n",
    "        self.vectordb = vectordb\n",
    "\n",
    "    def create_llm(self, temperature: float = 0.4):\n",
    "        \n",
    "            self.llm = LlamaCpp(model_path=\"../models/llama-2-7b-chat.ggmlv3.q4_0.bin\", verbose=True, n_ctx=2048, temperature=temperature)\n",
    "\n",
    "    def create_memory(self):\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "\n",
    "    def create_retriever(self):\n",
    "        self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "    def create_chat_session(self):\n",
    "\n",
    "        PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following pieces of context to answer the question at the end. \n",
    "        If you don't know the answer, just say that you don't know, don't try to male up an answer. \n",
    "        Use three sentences maximum. Keep answer as concise as possible. \n",
    "        Always say \"thanks for asking! at the end of the answer.\n",
    "        {context}\n",
    "        Question: {question}\n",
    "        Helpful Answer:\"\"\"\n",
    "\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "        self.qa = ConversationalRetrievalChain.from_llm(\n",
    "            self.llm,\n",
    "            retriever=self.retriever,\n",
    "            memory=self.memory,\n",
    "            combine_docs_chain_kwargs={'prompt': QA_CHAIN_PROMPT}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../docs/Anexo_2_Descricao_Desafios_caracteristicas_e_especificidades_dos_desafios']\n",
    "vectordb_path = \"../docs/chroma\"\n",
    "\n",
    "chat = ChatPDF(files, vectordb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.load()\n",
    "chat.split()\n",
    "chat.get_embeddings()\n",
    "chat.store()\n",
    "chat.create_llm()\n",
    "chat.create_memory()\n",
    "chat.create_retriever()\n",
    "chat.create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front end web app\n",
    "chat_history = []\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    chat_history = []\n",
    "    \n",
    "    def user(user_message, chat_history):\n",
    "        \n",
    "        # Get result from QA chain\n",
    "        result = chat.qa({\"question\": user_message, \"chat_history\": chat_history})\n",
    "        \n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append((user_message, result[\"answer\"]))\n",
    "\n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
